import google.generativeai as genai
import os
import json
import time
import requests
from datetime import datetime
from typing import Dict, Any, List, Optional
from bs4 import BeautifulSoup, Comment

class GeminiService:
    def __init__(self):
        # Get API keys from environment
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            raise ValueError("GEMINI_API_KEY not found in environment variables")
        
        self.unsplash_access_key = os.getenv("UNSPLASH_ACCESS_KEY")
        if not self.unsplash_access_key:
            print("WARNING: UNSPLASH_ACCESS_KEY not found, will use fallback images")
        
        # Configure Gemini
        genai.configure(api_key=api_key)
        
        # Get model name from environment or use default
        model_name = os.getenv("GEMINI_MODEL", "gemini-1.5-pro")
        print(f"Using Gemini model: {model_name}")
        
        # Initialize model with generation config
        self.model = genai.GenerativeModel(
            model_name=model_name,
            generation_config={
                "temperature": 0.8,
                "top_p": 0.95,
                "top_k": 40,
                "max_output_tokens": 64000,
            }
        )
        
        # Rate limiting
        self.last_request_time = 0
        self.min_request_interval = 1.0  # seconds
    
    def _check_rate_limits(self):
        """Simple rate limiting to avoid hitting API limits"""
        current_time = time.time()
        time_since_last = current_time - self.last_request_time
        if time_since_last < self.min_request_interval:
            time.sleep(self.min_request_interval - time_since_last)
        self.last_request_time = time.time()
    
    def _extract_search_keywords(self, product_type: str) -> str:
        """Use Gemini to extract English search keywords from product description"""
        try:
            prompt = f"""
            Translate this product description into 2-3 simple English keywords for stock photo search.
            Input: "{product_type}"
            
            Rules:
            1. Output ONLY the keywords separated by spaces
            2. No punctuation, no explanations
            3. Focus on the visual object (e.g. "warm roasted sweet potato lollipop" -> "lollipop candy dessert")
            """
            
            response = self.model.generate_content(prompt)
            keywords = response.text.strip()
            # Remove any accidental quotes or newlines
            keywords = keywords.replace('"', '').replace('\n', ' ')
            print(f"[{datetime.now()}] Translated '{product_type}' -> '{keywords}'")
            return keywords
        except Exception as e:
            print(f"[{datetime.now()}] Keyword extraction failed: {e}")
            # Fallback to simple replacement
            return product_type.replace("ì²œì—° ì¬ë£Œë¡œ ë§Œë“  ", "").replace("ìˆ˜ì œ ", "")

    def _get_unsplash_images(self, product_type: str, count: int = 8) -> List[str]:
        """Fetch product images from Unsplash API"""
        if not self.unsplash_access_key:
            print("No Unsplash key, using fallback")
            return []
        
        try:
            # Get optimized English keywords
            search_query = self._extract_search_keywords(product_type)
            
            url = "https://api.unsplash.com/photos/random"
            headers = {
                "Authorization": f"Client-ID {self.unsplash_access_key}"
            }
            params = {
                "query": search_query,
                "count": count,
                "orientation": "landscape"
            }
            
            print(f"[{datetime.now()}] Fetching {count} images from Unsplash for '{search_query}'...")
            response = requests.get(url, headers=headers, params=params, timeout=10)
            
            if response.status_code == 200:
                photos = response.json()
                
                image_urls = []
                
                # Handle both single photo and array responses (Unsplash API quirk)
                if isinstance(photos, list):
                    for i, photo in enumerate(photos):
                        try:
                            # Use 'raw' URL for maximum stability, with quality params
                            url = photo.get('urls', {}).get('raw', '')
                            if url:
                                # Add stable parameters to raw URL
                                stable_url = f"{url}&fm=jpg&q=80&w=1200&fit=max"
                                image_urls.append(stable_url)
                                print(f"[{datetime.now()}] Image {i+1}: {stable_url[:80]}...")
                        except Exception as e:
                            print(f"[{datetime.now()}] Failed to extract URL from photo {i+1}: {e}")
                            
                elif isinstance(photos, dict):
                    try:
                        url = photos.get('urls', {}).get('raw', '')
                        if url:
                            stable_url = f"{url}&fm=jpg&q=80&w=1200&fit=max"
                            image_urls.append(stable_url)
                            print(f"[{datetime.now()}] Single image: {stable_url[:80]}...")
                    except Exception as e:
                        print(f"[{datetime.now()}] Failed to extract single photo URL: {e}")
                
                # Filter out empty URLs
                image_urls = [url for url in image_urls if url and len(url) > 50]
                
                print(f"[{datetime.now()}] Successfully processed {len(image_urls)} valid images from Unsplash")
                
                if len(image_urls) < count:
                    print(f"[{datetime.now()}] WARNING: Only got {len(image_urls)} valid images, requested {count}")
                    
                return image_urls
            else:
                print(f"[{datetime.now()}] Unsplash API error: {response.status_code}")
                return []
                
        except Exception as e:
            print(f"[{datetime.now()}] Error fetching Unsplash images: {e}")
            return []
    
    def _clean_html(self, html_content: str) -> str:
        """
        Smart Filtering: Clean HTML to keep only structure and style-relevant tags.
        Removes noise like SVG paths, base64 images, and long text.
        """
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # 1. Remove completely useless tags
            for tag in soup(['noscript', 'iframe', 'object', 'embed']):
                tag.decompose()
            
            # 2. Remove comments
            for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):
                comment.extract()
            
            # 3. Clean SVG: Keep tag but remove paths (too long)
            for svg in soup.find_all('svg'):
                svg.clear() # Remove children (paths)
                svg.attrs = {k: v for k, v in svg.attrs.items() if k in ['class', 'id', 'width', 'height', 'viewbox']}
                svg.string = "SVG_ICON" # Placeholder
            
            # 4. Clean Images: Remove base64 src
            for img in soup.find_all('img'):
                src = img.get('src', '')
                if src.startswith('data:'):
                    img['src'] = 'BASE64_IMAGE_REMOVED'
                # Remove other attributes except critical ones
                img.attrs = {k: v for k, v in img.attrs.items() if k in ['src', 'class', 'id', 'alt']}
            
            # 5. Clean Scripts: Keep src (libraries), remove inline content
            for script in soup.find_all('script'):
                if script.get('src'):
                    # Keep external scripts (libraries)
                    script.string = "" 
                else:
                    # Remove inline scripts completely (usually logic, not style)
                    script.decompose()
            
            # 6. Clean Text: Truncate long text nodes
            for text in soup.find_all(string=True):
                if len(text) > 50 and text.parent.name not in ['style', 'script']:
                    text.replace_with(text[:50] + "...")
            
            # 7. Clean Attributes: Remove data-*, aria-*, on* events
            for tag in soup.find_all(True):
                attrs = dict(tag.attrs)
                for key in attrs:
                    if key.startswith('data-') or key.startswith('aria-') or key.startswith('on'):
                        del tag.attrs[key]
            
            return str(soup)
            
        except Exception as e:
            print(f"[{datetime.now()}] HTML cleaning failed: {e}")
            return html_content[:20000] # Fallback to truncation

    async def generate_website_content(self, product_type: str, reference_url: str, design_style: str, mode: str = 'smart') -> dict:
        print(f"[{datetime.now()}] Received generation request for: {product_type}")
        print(f"[{datetime.now()}] Design style (user request): {design_style}")
        print(f"[{datetime.now()}] Generation Mode: {mode.upper()}")
        
        self._check_rate_limits()

        # Fetch Reference URL content based on mode
        reference_html = ""
        fetch_success = False
        
        if reference_url and reference_url.strip() and mode != 'none':
            try:
                print(f"[{datetime.now()}] Fetching reference URL content: {reference_url}")
                headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
                resp = requests.get(reference_url, headers=headers, timeout=10)
                
                if resp.status_code == 200:
                    raw_html = resp.text
                    print(f"[{datetime.now()}] Fetched {len(raw_html)} chars")
                    
                    if mode == 'smart':
                        print(f"[{datetime.now()}] Applying Smart Filtering...")
                        reference_html = self._clean_html(raw_html)
                        print(f"[{datetime.now()}] Cleaned HTML length: {len(reference_html)} chars")
                    else: # mode == 'raw'
                        reference_html = raw_html[:60000] # Limit to 60k chars
                        print(f"[{datetime.now()}] Using Raw HTML (truncated to 60k)")
                    
                    fetch_success = True
                else:
                    print(f"[{datetime.now()}] Failed to fetch reference URL: {resp.status_code}")
            except Exception as e:
                print(f"[{datetime.now()}] Error fetching reference URL: {e}")
        
        # Fetch Unsplash images
        unsplash_images = self._get_unsplash_images(product_type, count=8)
        
        # Build image instructions
        if unsplash_images:
            image_instruction = f"""
        IMAGE REQUIREMENTS - USE THESE UNSPLASH URLS:
        You MUST use these pre-fetched Unsplash image URLs in your HTML:
        {chr(10).join([f'        - {url}' for url in unsplash_images])}
        
        Use different images for hero, product gallery, testimonials, etc.
        All images are landscape-oriented and professional quality.
        CRITICAL: Use ONLY these URLs, do not generate or modify them.
        """
        else:
            image_instruction = """
        IMAGE REQUIREMENTS - FALLBACK (Lorem Flickr):
        Use Lorem Flickr for images: https://loremflickr.com/800/600/keyword1,keyword2
        Choose keywords matching the product type (soap, cosmetics, food, etc.)
        """
        
        # Build reference section
        if reference_url and reference_url.strip():
            reference_source_info = ""
            if fetch_success and reference_html:
                reference_source_info = f"""
**ì°¸ê³  ì‚¬ì´íŠ¸ HTML ì†ŒìŠ¤ (ìŠ¤íƒ€ì¼ ë¶„ì„ìš©)**:
```html
{reference_html}
```
"""
            elif mode == 'none':
                 reference_source_info = "**ì°¸ê³  ì‚¬ì´íŠ¸ ì†ŒìŠ¤ ì œê³µ ì•ˆí•¨ (URLë§Œ ì°¸ì¡°)**"

            reference_section = f"""
## ë ˆí¼ëŸ°ìŠ¤ ë¶„ì„ (ìµœìš°ì„  - ë°˜ë“œì‹œ ìˆ˜í–‰)

âš ï¸ **í•„ìˆ˜**: ì•„ë˜ URL ë° ì œê³µëœ ì†ŒìŠ¤ì½”ë“œë¥¼ ë¶„ì„í•˜ì—¬ ìŠ¤íƒ€ì¼ì„ ì™„ë²½í•˜ê²Œ ë³µì œí•˜ì‹­ì‹œì˜¤.
**URL**: {reference_url}

{reference_source_info}

**ë°˜ë“œì‹œ ë¶„ì„í•´ì•¼ í•  í•­ëª©**:
1. **ì»¬ëŸ¬ íŒ”ë ˆíŠ¸**: ì •í™•í•œ HEX ì½”ë“œ ì¶”ì¶œ (Primary, Secondary, Accent, Background)
2. **í°íŠ¸**: ì‚¬ìš©ëœ í°íŠ¸ íŒ¨ë°€ë¦¬, í¬ê¸°, êµµê¸°
3. **ë ˆì´ì•„ì›ƒ**: Max-width, ì—¬ë°±, ì„¹ì…˜ êµ¬ì¡°, ê·¸ë¦¬ë“œ ì‹œìŠ¤í…œ
4. **ì¸í„°ë™ì…˜**: ì• ë‹ˆë©”ì´ì…˜ ì¢…ë¥˜, í˜¸ë²„ íš¨ê³¼, ì „í™˜ ì†ë„
5. **ì „ì²´ ë¶„ìœ„ê¸°**: ë””ìì¸ í†¤ì•¤ë§¤ë„ˆ

**êµ¬í˜„ ê·œì¹™**:
- ë ˆí¼ëŸ°ìŠ¤ ì‚¬ì´íŠ¸ì™€ **90% ì´ìƒ ë™ì¼í•œ ìŠ¤íƒ€ì¼**ë¡œ êµ¬í˜„
- ë™ì¼í•œ ì»¬ëŸ¬ ì½”ë“œ, ë™ì¼í•œ ë ˆì´ì•„ì›ƒ(mobile only/first í¬í•¨) êµ¬ì¡° ì‚¬ìš©
- ì‚¬ìš©ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ìˆë‹¤ë©´ ë™ì¼í•˜ê²Œ ì ìš© (GSAP, Swiper ë“±)
- ì‚¬ìš©ì ìš”ì²­ê³¼ ì¶©ëŒ ì‹œì—ë§Œ ì‚¬ìš©ì ìš”ì²­ ìš°ì„ 
"""
        else:
            reference_section = """
## ê¸°ë³¸ ë””ìì¸ ì°¸ì¡°
- ë ˆí¼ëŸ°ìŠ¤ ì—†ìŒ â†’ Awwwards E-commerce ìˆ˜ì¤€ í€„ë¦¬í‹° ì ìš©
- ì°¸ê³ : https://www.awwwards.com/websites/e-commerce/
- UI ì°¸ê³ : https://uiverse.io/elements
"""

        prompt = f"""
# Role
ì„¸ê³„ ìµœê³ ì˜ UI/UX ë””ìì´ë„ˆì´ì í”„ë¡ íŠ¸ì—”ë“œ ê°œë°œì

# Goal
í”„ë¦¬ë¯¸ì—„ ë§ˆì´í¬ë¡œ ì¸í„°ë™ì…˜ì´ ì ìš©ëœ í•œêµ­í˜• ì´ì»¤ë¨¸ìŠ¤ ì‚¬ì´íŠ¸ ìƒì„±
- **ìƒí’ˆ**: {product_type}
- **ë ˆí¼ëŸ°ìŠ¤**: {reference_url if reference_url else "ì—†ìŒ"}
- **ì‚¬ìš©ì ìš”ì²­ì‚¬í•­**: {design_style}

---

# 1. ë¶„ì„ (Analysis)

## ìƒí’ˆ ë¶„ì„
- í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ â†’ ì–´ìš¸ë¦¬ëŠ” ì»¬ëŸ¬ íŒ”ë ˆíŠ¸ ì„ ì •
{reference_section}

---

# 2. ìš°ì„ ìˆœìœ„ (Priority)

âš ï¸ **ìµœìš°ì„ **: ì‚¬ìš©ì ìš”ì²­ì‚¬í•­ "{design_style}"ì€ ë°˜ë“œì‹œ 100% êµ¬í˜„í•˜ì‹œì˜¤.

| ìˆœìœ„ | í•­ëª© | ì„¤ëª… |
|:---:|------|------|
| 1 | **ì‚¬ìš©ì ìš”ì²­ì‚¬í•­** | "{design_style}" - ë¬´ì¡°ê±´ ë°˜ì˜ |
| 2 | ë ˆí¼ëŸ°ìŠ¤ ìŠ¤íƒ€ì¼ | 80-90% ìœ ì‚¬í•˜ê²Œ êµ¬í˜„ |
| 3 | ê¸°ë³¸ ë””ìì¸ í‘œì¤€ | Awwwards ìˆ˜ì¤€ |

---

# 3. ì¡°ê±´ë³„ ì‹¤í–‰ ê·œì¹™

| ìƒí’ˆ | ë ˆí¼ëŸ°ìŠ¤ | ì²˜ë¦¬ |
|------|----------|------|
| test/í…ŒìŠ¤íŠ¸ | ìˆìŒ | ë ˆí¼ëŸ°ìŠ¤ í´ë¡  ì½”ë”© |
| test/í…ŒìŠ¤íŠ¸ | ì—†ìŒ | ìµœì†Œ ê¸°ë³¸ ì‚¬ì´íŠ¸ |
| ì¼ë°˜ | ìˆìŒ | ë ˆí¼ëŸ°ìŠ¤ ìŠ¤íƒ€ì¼ + ìƒí’ˆ ë°˜ì˜ |
| ì¼ë°˜ | ì—†ìŒ | ì°½ì˜ì  ë…ì°½ ë””ìì¸ |

---

# 4. ë””ìì¸ ì‹œìŠ¤í…œ

## ğŸ¯ ì‚¬ìš©ì ìš”ì²­ ìµœìš°ì„ 
**ì‚¬ìš©ì ìš”ì²­ì‚¬í•­ "{design_style}"ì´ ìˆë‹¤ë©´ â†’ ê·¸ ìš”ì²­ì„ 100% ë”°ë¥´ì‹œì˜¤.**
(ë‹¨ì¼ heroë¥¼ ì›í•˜ë©´ ë‹¨ì¼ heroë¡œ, ë¯¸ë‹ˆë©€ì„ ì›í•˜ë©´ ë¯¸ë‹ˆë©€ë¡œ)

## ğŸ“ ê¸°ë³¸ ë ˆì´ì•„ì›ƒ (ì‚¬ìš©ì ìš”ì²­ì´ ì—†ê±°ë‚˜ ì• ë§¤í•  ë•Œ)
ì‚¬ìš©ìê°€ íŠ¹ì • ë ˆì´ì•„ì›ƒì„ ì§€ì •í•˜ì§€ ì•Šì•˜ë‹¤ë©´, ë‹¤ìŒ ì¤‘ **ì°½ì˜ì ìœ¼ë¡œ ì„ íƒ**:

1. **ë©€í‹° ë°°ë„ˆí˜•** - Wì»¨ì…‰, ë¬´ì‹ ì‚¬ ìŠ¤íƒ€ì¼ (ë°°ë„ˆ 3~5ê°œ ê°€ë¡œ ë°°ì—´)
2. **ê·¸ë¦¬ë“œ ê°¤ëŸ¬ë¦¬í˜•** - Pinterest, 29cm ìŠ¤íƒ€ì¼ (ë‹¤ì–‘í•œ í¬ê¸° ì¹´ë“œ ë°°ì¹˜)
3. **ë§¤ê±°ì§„í˜•** - ì—ë””í† ë¦¬ì–¼ ëŠë‚Œ, í° ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸ ì¡°í•©
4. **ì¹´ë“œ ì¤‘ì‹¬í˜•** - ìƒí’ˆ ì¹´ë“œê°€ ì£¼ë¥¼ ì´ë£¨ëŠ” ê¹”ë”í•œ ê·¸ë¦¬ë“œ

âš ï¸ **ì£¼ì˜**: ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ ìš”ì²­í•˜ì§€ ì•ŠëŠ” í•œ, ë‹¨ìˆœíˆ í° ì´ë¯¸ì§€ í•˜ë‚˜ë§Œ ìˆëŠ” ë ˆì´ì•„ì›ƒì€ í”¼í•˜ì‹œì˜¤.

## í•„ìˆ˜ ìš”ì†Œ
- **ì»¬ëŸ¬**: ì¼ê´€ëœ íŒ”ë ˆíŠ¸ (ë ˆí¼ëŸ°ìŠ¤ ìˆìœ¼ë©´ ë™ì¼ ìƒ‰ìƒ)
- **í°íŠ¸**: ìƒí’ˆ/ë¶„ìœ„ê¸°ì— ì–´ìš¸ë¦¬ëŠ” Google Fonts
- **ì¸í„°ë™ì…˜**: ë¶€ë“œëŸ½ê³  í™”ë ¤í•œ ë§ˆì´í¬ë¡œ ì• ë‹ˆë©”ì´ì…˜ í•„ìˆ˜
- **í˜¸ë²„**: ë²„íŠ¼, ì¹´ë“œ, ì´ë¯¸ì§€ì— ì„¸ë ¨ëœ íš¨ê³¼

{image_instruction}

---

# 5. ì¶œë ¥ í˜•ì‹

```
<!DOCTYPE html>
<html lang="ko">
<head>...</head>
<body>...</body>
</html>
<<<METADATA_SEPARATOR>>>
{{"explanation": "...", "key_points": ["..."], "color_palette": ["..."]}}
```

---

# ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] ì‚¬ìš©ì ìš”ì²­ 100% ë°˜ì˜
- [ ] ë ˆí¼ëŸ°ìŠ¤ 80-90% ìœ ì‚¬ (í•´ë‹¹ ì‹œ)
- [ ] ë§ˆì´í¬ë¡œ ì¸í„°ë™ì…˜ ì ìš©
- [ ] ë ˆì´ì•„ì›ƒ ì œì•½ ì¤€ìˆ˜
- [ ] í”„ë¦¬ë¯¸ì—„ í€„ë¦¬í‹°
"""
        
        # Retry logic
        max_retries = 2
        last_error = None
        
        for attempt in range(max_retries):
            try:
                if attempt > 0:
                    print(f"[{datetime.now()}] Retry attempt {attempt + 1}/{max_retries}")
                    
                print(f"[{datetime.now()}] Sending request to Gemini API...")
                response = self.model.generate_content(prompt)
                print(f"[{datetime.now()}] Received response from Gemini")
                
                # Extract text
                raw_text = response.text.strip()
                print(f"[{datetime.now()}] Raw response length: {len(raw_text)} chars")
                
                # Clean up markdown fencing if present (sometimes Gemini still adds it)
                if raw_text.startswith("```html"):
                    raw_text = raw_text[7:]
                elif raw_text.startswith("```"):
                    raw_text = raw_text[3:]
                if raw_text.endswith("```"):
                    raw_text = raw_text[:-3]
                
                raw_text = raw_text.strip()
                
                # Parse using the separator
                separator = "<<<METADATA_SEPARATOR>>>"
                
                if separator in raw_text:
                    parts = raw_text.split(separator)
                    html_content = parts[0].strip()
                    json_part = parts[1].strip()
                    
                    # Clean up JSON part if it has markdown
                    if json_part.startswith("```json"):
                        json_part = json_part[7:]
                    if json_part.endswith("```"):
                        json_part = json_part[:-3]
                    json_part = json_part.strip()
                    
                    try:
                        metadata = json.loads(json_part, strict=False)
                        print(f"[{datetime.now()}] Successfully parsed metadata JSON")
                    except json.JSONDecodeError as je:
                        print(f"[{datetime.now()}] Metadata JSON parsing failed: {je}")
                        # Fallback metadata
                        metadata = {
                            "explanation": "ë””ìì¸ ìƒì„± ì™„ë£Œ (ë©”íƒ€ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨)",
                            "key_points": ["ë°˜ì‘í˜• ë””ìì¸", "ëª¨ë˜ ìŠ¤íƒ€ì¼", "ì¸í„°ë™í‹°ë¸Œ ìš”ì†Œ"],
                            "color_palette": ["#333333", "#ffffff"]
                        }
                    
                    # Construct final result
                    result = {
                        "html": html_content,
                        "explanation": metadata.get("explanation", ""),
                        "key_points": metadata.get("key_points", []),
                        "color_palette": metadata.get("color_palette", [])
                    }
                    
                    print(f"[{datetime.now()}] HTML length: {len(result['html'])} chars")
                    return result
                    
                else:
                    # Fallback: Maybe Gemini returned just JSON or just HTML?
                    # Try to parse as JSON (old way) just in case
                    try:
                        print(f"[{datetime.now()}] Separator not found, trying legacy JSON parse...")
                        # ... (legacy parsing logic omitted for brevity, assuming new prompt works)
                        # Actually, let's just treat the whole thing as HTML if it looks like HTML
                        if "<html" in raw_text.lower():
                            print(f"[{datetime.now()}] Treating entire response as HTML")
                            return {
                                "html": raw_text,
                                "explanation": "ìë™ ìƒì„±ëœ ë””ìì¸",
                                "key_points": [],
                                "color_palette": []
                            }
                        else:
                             raise ValueError("Response format invalid: Separator not found and not HTML")
                    except Exception as e:
                        raise ValueError(f"Failed to parse response: {str(e)}")

            except Exception as e:
                print(f"[{datetime.now()}] Error during generation: {type(e).__name__}: {str(e)}")
                if attempt == max_retries - 1:
                    raise
                last_error = e
                print(f"[{datetime.now()}] Will retry...")
                continue
        
        # If we get here, all retries failed
        raise ValueError(f"Failed to generate content after {max_retries} attempts. Last error: {last_error}")

# Create singleton instance
gemini_service = GeminiService()
